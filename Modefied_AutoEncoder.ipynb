{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610276de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "#plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "sns.reset_orig()\n",
    "sns.set()\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2c3627",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchvision \n",
    "from torchvision.datasets import CIFAR10, CocoDetection\n",
    "from torchvision import transforms\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba982ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "%load_ext tensorboard\n",
    "\n",
    "DATASET_PATH = \"data\"\n",
    "CHECKPOINT_PATH = \"saved_models/Autoencoder/\"\n",
    "\n",
    "pl.seed_everything(77)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "PATH_TO_DATA = \"data/COCO/train2017\"\n",
    "PATH_TO_ANN = \"data/COCO/annotations/instances_train2017.json\"\n",
    "PATH_TO_BGBBOX = \"data/COCO/annotations/coco_train_bg_bboxes.log\"\n",
    "\n",
    "print(\"Device: \",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158b8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoClsDataset(data.Dataset):\n",
    "    def __init__(self, img_dir, ann_file, bg_bboxes_file):\n",
    "        self.ann_file = PATH_TO_ANN\n",
    "        self.img_dir = PATH_TO_DATA\n",
    "        self.coco = COCO(self.ann_file)\n",
    "        self.bg_bboxes_file = bg_bboxes_file\n",
    "        self.transform = transforms.Compose([transforms.Resize((256, 256)),\n",
    "                                            transforms.ToTensor(),\n",
    "                                            transforms.Normalize((0.5,), (0.5,))])\n",
    "        cat_ids = self.coco.getCatIds()\n",
    "        categories = self.coco.dataset['categories']\n",
    "        self.id2cat = dict()\n",
    "        for category in categories:\n",
    "            self.id2cat[category['id']] = category['name']\n",
    "        self.id2cat[0] = 'background'\n",
    "        self.id2label = {category['id']:label + 1 for label, category in enumerate(categories)}\n",
    "        self.id2label[0] = 0\n",
    "        self.label2id = {v:k for v,k in self.id2label.items()}\n",
    "        tmp_ann_ids = self.coco.getAnnIds()\n",
    "        self.ann_ids = []\n",
    "        for ann_id in tmp_ann_ids:\n",
    "            ann = self.coco.loadAnns([ann_id])[0]\n",
    "            x, y, w, h = ann['bbox']\n",
    "            x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "            if ann['area'] <= 0 or w < 1 or h < 1 or ann['iscrowd']:\n",
    "                continue\n",
    "            self.ann_ids.append(ann_id)\n",
    "        self.bg_anns = self._load_bg_anns()\n",
    "        self._cal_num_dict()\n",
    "        print('total_length of dataset:', len(self))\n",
    "        \n",
    "    def _cal_num_dict(self):\n",
    "        self.num_dict = {}\n",
    "        for ann_id in self.ann_ids:\n",
    "            ann = self.coco.loadAnns([ann_id])[0]\n",
    "            cat = self.id2cat[ann['category_id']]\n",
    "            num = self.num_dict.get(cat, 0)\n",
    "            self.num_dict[cat] = num + 1\n",
    "        self.num_dict['background'] = len(self.bg_anns)\n",
    "    \n",
    "    def _load_bg_anns(self):\n",
    "        assert os.path.exists(self.bg_bboxes_file)\n",
    "        bg_anns = []\n",
    "        with open(self.bg_bboxes_file, 'r') as f:\n",
    "            line = f.readline()\n",
    "            while line:\n",
    "                if line.strip() == '':\n",
    "                    break\n",
    "                file_name, num = line.strip().split()\n",
    "                for _ in range(int(num)):\n",
    "                    bbox = f.readline()\n",
    "                    bbox = bbox.strip().split()\n",
    "                    bbox = [float(i) for i in bbox]\n",
    "                    w = bbox[2] - bbox[0] + 1\n",
    "                    h = bbox[3] - bbox[1] + 1\n",
    "                    bbox[2], bbox[3] = w, h\n",
    "                    ann = dict(\n",
    "                        file_name=file_name,\n",
    "                        bbox=bbox)\n",
    "                    bg_anns.append(ann)\n",
    "                line = f.readline()\n",
    "        return bg_anns\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ann_ids) + len(self.bg_anns)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx < len(self.ann_ids):\n",
    "            ann = self.coco.loadAnns([self.ann_ids[idx]])[0]\n",
    "\n",
    "            cat_id = ann['category_id']\n",
    "            label = self.id2label[cat_id]\n",
    "\n",
    "            img_meta = self.coco.loadImgs(ann['image_id'])[0]\n",
    "            img_path = os.path.join(self.img_dir, img_meta['file_name'])\n",
    "        else:\n",
    "            ann = self.bg_anns[idx - len(self.ann_ids)]\n",
    "\n",
    "            label = 0\n",
    "\n",
    "            img_path = os.path.join(self.img_dir, ann['file_name'])\n",
    "\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        x, y, w, h = ann['bbox']\n",
    "        x, y, w, h = int(x), int(y), int(w), int(h)\n",
    "        img = img.crop((x, y, x + w - 1, y + h - 1))\n",
    "\n",
    "        # save_img = img.resize((224, 224), Image.BILINEAR)\n",
    "        # save_img.save('test.jpg')\n",
    "\n",
    "        try:\n",
    "            img = self.transform(img)\n",
    "        except:\n",
    "            print(img.mode)\n",
    "            exit(0)\n",
    "        if label != 0:\n",
    "            label = 1\n",
    "        tmp_label = torch.zeros(2)\n",
    "        tmp_label[label] = 1\n",
    "        return img, tmp_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a589188",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_dataset = CocoClsDataset(img_dir=PATH_TO_DATA, # takes at least 10 sec to execute\n",
    "                          ann_file=PATH_TO_ANN,\n",
    "                          bg_bboxes_file=PATH_TO_BGBBOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7ade6",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = coco_dataset.__getitem__(130000)\n",
    "plt.imshow(images.permute(1, 2, 0))\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bdb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coco_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df552c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, _ = torch.utils.data.random_split(coco_dataset, [100000, 1599804])\n",
    "train_set, val_set = torch.utils.data.random_split(train_data, [80000, 20000])\n",
    "val_set, test_set = torch.utils.data.random_split(val_set, [15000, 5000])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=100, shuffle=True, drop_last=True, pin_memory=True, num_workers=4)\n",
    "val_loader = DataLoader(val_set, batch_size=100, shuffle=False, drop_last=False, num_workers=4)\n",
    "test_loader = DataLoader(test_set, batch_size=100, shuffle=False, drop_last=False, num_workers=4)\n",
    "\n",
    "def get_train_images(num):\n",
    "    return torch.stack([train_data[i][0] for i in range(num)], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac3b67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52c64f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d439f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(val_set[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abfac72",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                num_input_channels : int, \n",
    "                base_channel_size : int, \n",
    "                latent_dim : int, \n",
    "                act_fn : object = nn.GELU):\n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(num_input_channels, c_hid, kernel_size=3, padding=1, stride=2), # 256x256 ==> 128x128\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1), \n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1, stride=2), # 128x128 ==> 64x64\n",
    "            act_fn(), \n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1), # \n",
    "            act_fn(), \n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1, stride=2), # 64x64 ==> 32x32\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),#\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1, stride=2), # 32x32 ==> 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1), # dimension won't change\n",
    "            act_fn(), \n",
    "            nn.Conv2d(c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 16x16 ==> 8x8\n",
    "            act_fn(), \n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1), #\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1, stride=2), # 8x8 ==> 4x4\n",
    "            act_fn(),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(2*16*c_hid, latent_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347d5793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "                 num_input_channels : int, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 act_fn : object = nn.GELU):\n",
    "        \n",
    "        super().__init__()\n",
    "        c_hid = base_channel_size\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 2*16*c_hid), \n",
    "            act_fn()\n",
    "        )\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2*c_hid, 2*c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 4x4==>8x8\n",
    "            act_fn(),\n",
    "            nn.Conv2d(2*c_hid, 2*c_hid, kernel_size=3, padding=1), \n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(2*c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 8x8 ==> 16x16\n",
    "            act_fn(),\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(), \n",
    "            nn.ConvTranspose2d(c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 16x16 ==> 32x32\n",
    "            act_fn(), \n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1), \n",
    "            act_fn(), \n",
    "            nn.ConvTranspose2d(c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), # 32x32 ==> 64x64\n",
    "            act_fn(), \n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, c_hid, kernel_size=3, output_padding=1, padding=1, stride=2), #64 ==> 128\n",
    "            nn.Conv2d(c_hid, c_hid, kernel_size=3, padding=1),\n",
    "            act_fn(),\n",
    "            nn.ConvTranspose2d(c_hid, num_input_channels, kernel_size=3, output_padding=1, padding=1, stride=2), #128-->256\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear(x)\n",
    "        x = x.reshape(x.shape[0], -1, 4, 4)\n",
    "        x = self.net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f0961a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(pl.LightningModule):\n",
    "    def __init__(self, \n",
    "                 base_channel_size : int, \n",
    "                 latent_dim : int, \n",
    "                 encoder_class : object = Encoder,\n",
    "                 decoder_class : object = Decoder, \n",
    "                 num_input_channels : int = 3,\n",
    "                 width : int = 256, \n",
    "                 height : int = 256\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.encoder = encoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.decoder = decoder_class(num_input_channels, base_channel_size, latent_dim)\n",
    "        self.example_input_array = torch.zeros(2, num_input_channels, width, height)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        x_hat = self.decoder(z)\n",
    "        return x_hat\n",
    "    \n",
    "    def _get_reconstruction_loss(self, batch, mode=\"mse\"):\n",
    "        x, _ = batch\n",
    "        x_hat = self.forward(x)\n",
    "        if mode == \"mse\":\n",
    "            loss = F.mse_loss(x, x_hat, reduction=\"none\")\n",
    "        else:\n",
    "            loss = F.kl_div(x, x_hat, reduction=\"none\")\n",
    "        loss = loss.sum(dim=[1,2,3]).mean(dim=[0])\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                         mode = 'min', \n",
    "                                                         factor=0.2, \n",
    "                                                         patience=20, \n",
    "                                                         min_lr=5e-5)\n",
    "        return {\"optimizer\":optimizer, \"lr_scheduler\":scheduler, \"monitor\":\"val_loss\"}\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('val_loss', loss)\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss = self._get_reconstruction_loss(batch)\n",
    "        self.log('test_loss', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd2e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imgs(img1, img2, title_prefix=\"\"):\n",
    "    loss = F.mse_loss(img1, img2, reduction=\"sum\")\n",
    "    grid = torchvision.utils.make_grid(torch.stack([img1, img2], dim=0), nrow=2, normalize=True, range=(-1,1))\n",
    "    grid = grid.permute(1,2,0)\n",
    "    plt.figure(figsize=(4,2))\n",
    "    plt.title(f\"{title_prefix} Loss : {loss.item():4.2f}\")\n",
    "    plt.imshow(grid)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for i in range(2):\n",
    "    # Load example image\n",
    "    img, _ = train_data[i]\n",
    "    img_mean = img.mean(dim=[1,2], keepdims=True)\n",
    "\n",
    "    # Shift image by one pixel\n",
    "    SHIFT = 1\n",
    "    img_shifted = torch.roll(img, shifts=SHIFT, dims=1)\n",
    "    img_shifted = torch.roll(img_shifted, shifts=SHIFT, dims=2)\n",
    "    img_shifted[:,:1,:] = img_mean\n",
    "    img_shifted[:,:,:1] = img_mean\n",
    "    compare_imgs(img, img_shifted, \"Shifted -\")\n",
    "\n",
    "    # Set half of the image to zero\n",
    "    img_masked = img.clone()\n",
    "    img_masked[:,:img_masked.shape[1]//2,:] = img_mean\n",
    "    compare_imgs(img, img_masked, \"Masked -\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateCallback(pl.Callback):\n",
    "    \n",
    "    def __init__(self, input_imgs, every_n_epochs=1):\n",
    "        super().__init__()\n",
    "        self.input_imgs = input_imgs\n",
    "        self.every_n_epochs = every_n_epochs\n",
    "        \n",
    "    def on_epoch_end(self, trainer, pl_module):\n",
    "        if trainer.current_epoch % self.every_n_epochs == 0:\n",
    "            input_imgs = self.input_imgs.to(pl_module.device)\n",
    "            with torch.no_grad():\n",
    "                pl_module.eval()\n",
    "                reconst_imgs = pl_module(input_imgs)\n",
    "                pl_module.train()\n",
    "            imgs = torch.stack([input_imgs, reconst_imgs], dim = 1).flatten(0,1)\n",
    "            grid = torchvision.utils.make_grid(imgs, nrow=2, normalize=True, range=(-1,1))\n",
    "            trainer.logger.experiment.add_image(\"Reconstructions\", grid, global_step=trainer.global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4debdd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_coco(latent_dim):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, \"coco80_%i\" %latent_dim),\n",
    "                         gpus=1, \n",
    "                         max_epochs=300, \n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True), \n",
    "                                    GenerateCallback(get_train_images(8), every_n_epochs=10),\n",
    "                                    LearningRateMonitor(\"epoch\")])\n",
    "    trainer.logger._log_graph = True\n",
    "    trainer.logger._default_hp_metric = None\n",
    "    \n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"coco80_%i.ckpt\" % latent_dim)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading ...\")\n",
    "        model = Autoencoder.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        model = Autoencoder(base_channel_size=64, latent_dim=latent_dim)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "    # Testing the model on validation data\n",
    "    val_result = trainer.test(model, test_dataloaders=val_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=test_loader, verbose=False)\n",
    "    result = {\"test\":test_result, \"val\":val_result}\n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b834251",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict = {}\n",
    "for latent_dim in [384,512]:\n",
    "    model_ld, result_ld = train_coco(latent_dim)\n",
    "    model_dict[latent_dim] = {\"model\": model_ld, \"result\": result_ld} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac6e0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
